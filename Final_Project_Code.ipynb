{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPi5LHXcI+zoOMRAUkoRlFp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# RadioXenon Beta-Gamma Spectra Isotope Concentration Analysis\n","This research will focus on applying a few different neural networks to train a model that can accurately predict concentrations of the four main radioxenon isotopes/isomers from the beta-gamma spectra. The three models are a neural network (NN), a convolutional neural network (CNN), and then a hybrid network that is a mix of a convolutional neural network and direct input. This research will also test the behavior of the models under two different loss functions, Mean Squared Error (MSE) and Huber Loss.  Due to the complexity of the data utilized, this research will  focus on the models' performance on a Monte Carlo synthetically generated data set. Testing the models' behavior with a real data set is out of scope for this project.\n","\n","It is meant to be run in Google Colab. I also did substantial work with the 'real data' from INL but I could not get the data to really be in the same form as the PNNL data. That work was super messy and so I just removed it from this code that I would post in relation to my work on the final project. The code I inherited had a ton of hard coded values that I was not sure where they came from or why, so I chose to just work with the PNNL data."],"metadata":{"id":"XYIcllydd3O7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KJvgueB2cocZ"},"outputs":[],"source":["#Connect the Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["WorkingDirectory= r'/content/drive/MyDrive/Colab Notebooks/PHYS570AI/XeData/sims3'"],"metadata":{"id":"p3sIk9rteROj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Import Packages\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors\n","import math\n","import os, os.path\n","import scipy.optimize as opt\n","import scipy.integrate as integrate\n","from copy import deepcopy\n","from itertools import permutations\n","from numba import guvectorize, vectorize, cuda\n","from concurrent.futures import ThreadPoolExecutor\n","import time\n","import threading\n","from matplotlib import image\n","import mpmath as mp\n","import matplotlib as mpl\n","from datetime import datetime\n","import scipy.optimize as opt\n","from copy import deepcopy\n","from tqdm import tqdm\n","\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","%matplotlib inline\n","seed = 0\n","np.random.seed(seed)\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l1\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.callbacks import ReduceLROnPlateau\n","\n","from sklearn.decomposition import PCA\n","import seaborn as sns\n","\n","import os\n","import re\n","import time\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","from tensorflow.keras.losses import Huber\n"],"metadata":{"id":"lDo0J-0jeUJQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#functions\n","\n","# -------------------------\n","# TestAccumulator\n","# -------------------------\n","class TestAccumulator:\n","    def __init__(self, workingDir):\n","        self.results = []\n","        self.workingDir = str(workingDir)\n","\n","    def execute(self, use_tqdm=True, save_cache=None):\n","        \"\"\"\n","        Walk self.workingDir, build XeFile objects and cache if requested.\n","        - use_tqdm: show progress bar\n","        - save_cache: path (str) to .npz file to save histograms+concentrations\n","        \"\"\"\n","        start = time.time()\n","        files = []\n","        for root, _, fnames in os.walk(self.workingDir):\n","            for fname in fnames:\n","                files.append(os.path.join(root, fname))\n","\n","        loader = (tqdm(files, desc=\"Processing XeFiles\") if use_tqdm else files)\n","\n","        self.results = []\n","        for p in loader:\n","            try:\n","                self.results.append(XeFile(p))\n","            except Exception as e:\n","                # don't stop the whole run for one bad file; log it instead\n","                print(f\"Failed to load {p}: {e}\")\n","\n","        elapsed = time.time() - start\n","        print(f\"Processed {len(self.results)} files in {elapsed:.1f} s\")\n","\n","        if save_cache:\n","            # Save arrays for quick reload: histograms and concentrations\n","            hist_list = []\n","            conc_list = []\n","            for r in self.results:\n","                # ensure histogram is numpy array\n","                hist_list.append(np.asarray(r['histogram']))\n","                conc_list.append(np.asarray(r.concentration, dtype=np.float64))\n","            # Stack histograms if shapes agree; otherwise save as object array\n","            try:\n","                hist_stack = np.stack(hist_list)\n","            except Exception:\n","                hist_stack = np.array(hist_list, dtype=object)\n","            conc_stack = np.vstack(conc_list)\n","            np.savez_compressed(save_cache, histograms=hist_stack, concentrations=conc_stack)\n","            print(f\"Saved cache to {save_cache}\")\n","\n","        return self.results\n","\n","\n","# -------------------------\n","# XeFile\n","# -------------------------\n","class XeFile(dict):\n","    \"\"\"\n","    Minimal API preserved:\n","      - instance behaves like dict with keys for blocks (renamed)\n","      - attributes: filename, concentration (4-tuple list), getEnergies(), histogram, histogramUncertainty, etc.\n","    \"\"\"\n","    # patterns used for concentration extraction\n","    FLOAT_RE = re.compile(r'(?P<num>\\d+(?:\\.\\d+)?)')\n","    # useful labels to detect compound filenames; kept to preserve legacy mapping logic\n","    COMPOUND_TAGS = [\n","        \"Xe131m_Xe133m_Xe133_Xe135\",\n","        \"Xe131m_Xe133m_Xe133\",\n","        \"Xe131m_Xe133_Xe135\",\n","        \"Xe133andXe133m\",\n","        \"Xe131m_Xe135\",\n","        \"Xe131m_Xe133\",\n","        \"Xe133_Xe135\",\n","        \"Xe133m_Xe133_Xe135\",\n","        \"Xe131m\",\n","        \"Xe133\",\n","        \"Xe135\",\n","    ]\n","\n","    def __init__(self, arg):\n","        dict.__init__(self)\n","        if isinstance(arg, str):\n","            self.filename = arg\n","            self.loadFromPHD(arg)\n","        elif isinstance(arg, dict):\n","            # preserve old behavior if dict passed\n","            dict.update(self, arg)\n","            # best-effort set concentration if present\n","            self.concentration = list(dict.get(self, \"concentration\", [0, 0, 0, 0]))\n","        else:\n","            raise Exception(\"XeFile requires either a string file path or a dictionary as its argument.\")\n","\n","    # small fast numeric conversion; tries to avoid exceptions where possible\n","    @staticmethod\n","    def _string_to_num_token(tok):\n","        # quick heuristic: has decimal point or exponent -> float\n","        if any(c in tok for c in \".eE\"):\n","            try:\n","                return float(tok)\n","            except ValueError:\n","                return tok\n","        else:\n","            # integer-like token\n","            try:\n","                return int(tok)\n","            except ValueError:\n","                try:\n","                    return float(tok)\n","                except ValueError:\n","                    return tok\n","\n","    def _parse_line_tokens(self, line):\n","        # split by whitespace and convert tokens\n","        parts = line.strip().split()\n","        return [self._string_to_num_token(tok) for tok in parts]\n","\n","    def loadFromPHD(self, filePath):\n","        \"\"\"\n","        Stream the file once, build raw block text, then parse only what is required.\n","        Preserves full parsing behavior.\n","        \"\"\"\n","        # helper to get filename base\n","        filename = os.path.basename(filePath)\n","\n","        # -----------------------\n","        # 1) extract concentration(s)\n","        # -----------------------\n","        # default\n","        conc1 = conc2 = conc3 = conc4 = 0.0\n","\n","        # find floating numbers in filename\n","        nums = [float(m.group('num')) for m in self.FLOAT_RE.finditer(filename)]\n","\n","\n","        # Intent: if a specific tag is present, map positions appropriately\n","        # Assume numbers found correspond to the concentrations in order.\n","        if any(tag in filePath for tag in [\"Xe131m_Xe133m_Xe133_Xe135\"]):\n","            if len(nums) >= 4:\n","                conc1, conc2, conc3, conc4 = nums[:4]\n","        elif any(tag in filePath for tag in [\"Xe131m_Xe133m_Xe133\", \"Xe131m_Xe133_Xe135\",\n","                                             \"Xe131m_Xe135\", \"Xe131m_Xe133\", \"Xe133_Xe135\",\n","                                             \"Xe133m_Xe133_Xe135\"]):\n","            if len(nums) >= 4:\n","                conc1, conc2, conc3, conc4 = nums[:4]\n","        elif \"Xe133andXe133m\" in filePath:\n","            # special two-number filename form\n","            if len(nums) >= 2:\n","                conc2, conc3 = nums[:2]\n","        elif \"Xe131m\" in filePath and not (\"Xe131m_\" in filePath):\n","            # single isotope filename, like \"12nXYZ.phd\" -> interpreted earlier as single conc\n","            if nums:\n","                conc1 = nums[0]\n","        elif \"Xe133\" in filePath and not (\"Xe133_\" in filePath):\n","            if nums:\n","                conc2 = nums[0]\n","        elif \"Xe135\" in filePath and not (\"Xe135_\" in filePath):\n","            if nums:\n","                conc4 = nums[0]\n","        else:\n","            # fallback: if 4 numbers present assume them\n","            if len(nums) >= 4:\n","                conc1, conc2, conc3, conc4 = nums[:4]\n","            elif len(nums) == 2:  # maybe a two-isotope file\n","                conc2, conc3 = nums[:2]\n","\n","        self.concentration = [conc1, conc2, conc3, conc4]\n","\n","        # -----------------------\n","        # 2) stream file into blocks (single pass)\n","        # -----------------------\n","        blocks_raw = {}         # map block_name -> list[str] (body lines)\n","        current_key = None\n","        with open(filePath, 'r') as fh:\n","            for raw_line in fh:\n","                line = raw_line.rstrip('\\n')\n","                if not line:\n","                    continue\n","                if line.startswith('#'):\n","                    # new block header: everything after '#' stripped\n","                    current_key = line[1:].strip()\n","                    # initialize block list (may get overwritten later)\n","                    blocks_raw[current_key] = []\n","                else:\n","                    if current_key is None:\n","                        # lines before first '#' are ignored by original; we skip\n","                        continue\n","                    blocks_raw[current_key].append(line)\n","\n","        # store filepath\n","        self.update({'filePath': filePath})\n","\n","        # -----------------------\n","        # 3) convert raw blocks into lists of tokenized lines (numbers/strings)\n","        # -----------------------\n","        # Reuse parsed tokens to avoid repeated splits later\n","        parsed_blocks = {}\n","        for k, lines in blocks_raw.items():\n","            parsed = [self._parse_line_tokens(l) for l in lines if l.strip() != '']\n","            parsed_blocks[k] = parsed\n","\n","        # attach parsed blocks as original code would have (but keep parsed tokens)\n","        # Keep tokenized lists.\n","        self.update(parsed_blocks)\n","\n","        # -----------------------\n","        # 4) Build structured blocks per original 'blocks' map\n","        # -----------------------\n","        blocks_map = [\n","            {'oldName': 'g_Energy', 'newName': 'gammaEnergyCalibration', 'keys': ['energy', ['channel', ['value','uncertainty']]]},\n","            {'oldName': 'b_Energy', 'newName': 'betaEnergyCalibration', 'keys': ['energy', 'type', ['channel',['value','uncertainty']]]},\n","            {'oldName': 'g_Resolution', 'newName': 'gammaResolution', 'keys': ['energy', ['fwhm', ['value','uncertainty']]]},\n","            {'oldName': 'b_Resolution', 'newName': 'betaResolution', 'keys': ['energy', ['fwhm', ['value','uncertainty']]]},\n","            {'oldName': 'g_Efficiency', 'newName': 'gammaEfficiency', 'keys': ['energy', ['efficiency', ['value','uncertainty']]]},\n","            {'oldName': 'b-gEfficiency', 'newName': 'betaEfficiency', 'keys': ['nuclide', 'roi', ['efficiency', ['value','uncertainty']]]},\n","            {'oldName': 'Ratios', 'newName': 'ratios', 'keys': ['identifier', 'roiHigher', 'roiLower', ['ratio', ['value','uncertainty']]]},\n","            {'oldName': 'ROI_Limits', 'newName': 'roiEnergyRange', 'keys': ['roi', 'blower', 'bupper', 'glower', 'gupper']},\n","        ]\n","\n","        for b in blocks_map:\n","            old = b['oldName']\n","            new = b['newName']\n","            raw = self.pop(old, [])  # may be [] if missing\n","            out_list = []\n","            for line in raw:\n","                lineDict = {}\n","                for i, key in enumerate(b['keys']):\n","                    if isinstance(key, list):\n","                        # nested map: key[0] -> dict of subkeys from key[1]\n","                        subname = key[0]\n","                        subkeys = key[1]\n","                        # map subkeys with consecutive items from line\n","                        subdict = {subkeys[j]: line[i + j] for j in range(len(subkeys))}\n","                        lineDict[subname] = subdict\n","                    else:\n","                        lineDict[key] = line[i] if i < len(line) else None\n","                out_list.append(lineDict)\n","            self[new] = out_list\n","\n","        # -----------------------\n","        # 5) clean ROI block into dict-of-dicts\n","        # -----------------------\n","        roi_raw = self.pop('roiEnergyRange', [])\n","        try:\n","            self['roiEnergyRange'] = {\n","                roi['roi']: {'beta': {'lower': roi['blower'], 'upper': roi['bupper']},\n","                             'gamma': {'lower': roi['glower'], 'upper': roi['gupper']}}\n","                for roi in roi_raw\n","            }\n","        except Exception:\n","            # fallback empty\n","            self['roiEnergyRange'] = {}\n","\n","        # -----------------------\n","        # 6) Build spectra (beta/gamma) as pandas Series\n","        #    This requires getEnergies() to have been run first because it uses calibration blocks\n","        # -----------------------\n","        # Ensure energy calibrations exist; if not, leave spectra empty\n","        if ('gammaEnergyCalibration' in self) and ('betaEnergyCalibration' in self):\n","            # compute energies & uncertainties\n","            self.getEnergies()\n","            spectraType = {'beta': 'b', 'gamma': 'g'}\n","            for key in spectraType:\n","                # original code popped e.g. 'b_Spectrum' and took [1:]\n","                blockname = spectraType[key] + '_Spectrum'\n","                spectrum = self.pop(blockname, [])[1:]\n","                # original code removed first item of each line (index), so drop index token if present\n","                # and flatten remaining tokens into one big array\n","                if spectrum:\n","                    cleaned = []\n","                    for line in spectrum:\n","                        # drop first token (index) if it exists\n","                        if line:\n","                            cleaned.append(line[1:])\n","                    if cleaned:\n","                        arr = np.concatenate(cleaned).ravel()\n","                        self[key + 'Spectrum'] = pd.Series(arr, index=self[key + 'Energy'].values)\n","                    else:\n","                        self[key + 'Spectrum'] = pd.Series(dtype=float)\n","                else:\n","                    self[key + 'Spectrum'] = pd.Series(dtype=float)\n","        else:\n","            # keep placeholders if no calibration present\n","            self['betaSpectrum'] = pd.Series(dtype=float)\n","            self['gammaSpectrum'] = pd.Series(dtype=float)\n","\n","        # -----------------------\n","        # 7) Build histogram DataFrame & uncertainties\n","        # -----------------------\n","        hist_raw = self.pop('Histogram', [])\n","        if hist_raw:\n","            # original used histogram[1:-1] - keep same indexing behavior\n","            if len(hist_raw) >= 2:\n","                histogram_rows = hist_raw[1:-1]\n","            else:\n","                histogram_rows = hist_raw\n","\n","            # convert to numpy 2D array of floats (guard against ragged)\n","            try:\n","                N = np.array(histogram_rows, dtype=float)\n","            except Exception:\n","                # ragged -> convert each row individually into an array then stack with padding\n","                rows = [np.asarray(r, dtype=float) for r in histogram_rows]\n","                maxc = max(len(r) for r in rows)\n","                padded = np.vstack([np.pad(r, (0, maxc - len(r)), 'constant', constant_values=0.0) for r in rows])\n","                N = padded\n","\n","            # dx, dy and uncertainties come from previously built keys (they are numbers in the original parser)\n","            # original code used: dx = self['betaEnergyDelta']; dy = self['gammaEnergyDelta']\n","            dx = float(self.get('betaEnergyDelta', 1.0))\n","            dy = float(self.get('gammaEnergyDelta', 1.0))\n","            sigma_dx = float(self.get('betaEnergyDeltaUncertainty', 0.0))\n","            sigma_dy = float(self.get('gammaEnergyDeltaUncertainty', 0.0))\n","\n","            # preserve old DataFrame semantics for indexing and columns (gamma energies are rows, beta energies columns)\n","            # But store histogram as numpy array for speed; keep DataFrame if user expects it in self\n","            try:\n","                beta_energy_vals = self['betaEnergy'].values if isinstance(self['betaEnergy'], pd.Series) else np.arange(N.shape[1])\n","            except Exception:\n","                beta_energy_vals = np.arange(N.shape[1])\n","            try:\n","                gamma_energy_vals = self['gammaEnergy'].values if isinstance(self['gammaEnergy'], pd.Series) else np.arange(N.shape[0])\n","            except Exception:\n","                gamma_energy_vals = np.arange(N.shape[0])\n","\n","            # store both numpy and pandas-like object as in original code\n","            self['histogram'] = pd.DataFrame(N / (dx * dy),\n","                                             index=gamma_energy_vals,\n","                                             columns=beta_energy_vals)\n","            # also keep a numpy mirror for faster numeric ops\n","            self.histogram = np.array(N / (dx * dy))\n","\n","            # compute uncertainty preserving original formula\n","            sqrtN = np.sqrt(N)\n","            with np.errstate(divide='ignore', invalid='ignore'):\n","                term1 = (sqrtN / (dx * dy)) ** 2\n","                term2 = (-N * sigma_dx / (dx ** 2 * dy)) ** 2\n","                term3 = (-N * sigma_dy / (dx * dy ** 2)) ** 2\n","                term4 = 0.1 / (dx * dy)\n","                hist_unc = np.sqrt(term1 + term2 + term3 + term4)\n","\n","            self['histogramUncertainty'] = pd.DataFrame(hist_unc,\n","                                                        index=gamma_energy_vals,\n","                                                        columns=beta_energy_vals)\n","        else:\n","            self['histogram'] = pd.DataFrame()\n","            self.histogram = np.array([])\n","            self['histogramUncertainty'] = pd.DataFrame()\n","\n","        return self\n","\n","    # -----------------------\n","    # getEnergies method\n","    # -----------------------\n","    def getEnergies(self, plotEnergyChannelFit=False):\n","        \"\"\"\n","        Computes self['betaEnergy'], self['gammaEnergy'], their uncertainties, min/max etc.\n","        Returns (betaEnergySeries, gammaEnergySeries)\n","        \"\"\"\n","        for energyType in ['betaEnergy', 'gammaEnergy']:\n","            calib_key = energyType + 'Calibration'\n","            if calib_key not in self or not self[calib_key]:\n","                # no calibration; create default linear mapping\n","                chan = pd.Series(range(1, 257), index=range(1, 257))\n","                self[energyType] = chan\n","                self[energyType + 'Uncertainty'] = pd.Series(np.zeros_like(chan), index=chan.index)\n","                self[energyType + 'Delta'] = 1.0\n","                self[energyType + 'DeltaUncertainty'] = 0.0\n","                self[energyType + 'Min'] = float(self[energyType].iloc[0])\n","                self[energyType + 'Max'] = float(self[energyType].iloc[-1])\n","                continue\n","\n","            energies = np.array([data['energy'] for data in self[calib_key]], dtype=float)\n","            channels = np.array([data['channel']['value'] for data in self[calib_key]], dtype=float)\n","            uncertainties = np.array([data['channel'].get('uncertainty', 1e-6) or 1e-6 for data in self[calib_key]], dtype=float)\n","\n","            # linear fit E = dE*chan + E0 with weights = 1/uncertainties\n","            # use numpy.polyfit with cov\n","            (dE, E0), covariance = np.polyfit(channels, energies, 1, w=1.0 / uncertainties, cov=True)\n","\n","            self[energyType + 'Delta'] = float(dE)\n","            self[energyType + 'DeltaUncertainty'] = float(np.sqrt(covariance[0, 0]))\n","            chan = pd.Series(range(1, 257), index=range(1, 257))\n","            self[energyType] = chan * self[energyType + 'Delta'] + E0\n","            # uncertainty formula preserved\n","            cov00 = covariance[0, 0]\n","            cov11 = covariance[1, 1]\n","            cov01 = covariance[0, 1]\n","            self[energyType + 'Uncertainty'] = np.sqrt((chan * cov00) ** 2 + cov11 ** 2 + 2 * chan * abs(cov01))\n","            self[energyType + 'Min'] = float(self[energyType].iloc[0])\n","            self[energyType + 'Max'] = float(self[energyType].iloc[-1])\n","\n","            if plotEnergyChannelFit:\n","                plt.figure(figsize=(4, 4))\n","                x = np.arange(1, 101)\n","                y = self[energyType][:100]\n","                yminus = y - self[energyType + 'Uncertainty'][:100]\n","                yplus = y + self[energyType + 'Uncertainty'][:100]\n","                plt.plot(x, y)\n","                plt.plot(x, yminus, linestyle='dashed')\n","                plt.plot(x, yplus, linestyle='dashed')\n","                plt.errorbar(channels, energies, yerr=uncertainties, fmt='o')\n","                plt.xlabel('Channel')\n","                plt.ylabel(energyType)\n","                plt.show()\n","\n","        return (self['betaEnergy'], self['gammaEnergy'])\n","\n","\n","# -------------------------\n","# Helper plotting functions\n","# -------------------------\n","def plotProjection(data, spectrumObject=None, figureScale=1, resolution=500):\n","    # (kept almost identical to original but minor robustness updates)\n","    if isinstance(spectrumObject, list):\n","        x = np.linspace(data.columns[0], data.columns[-1], resolution)\n","        y = np.linspace(data.index[0], data.index[-1], resolution)\n","        xmesh, ymesh = np.meshgrid(x, y)\n","        spectrumObject = pd.DataFrame(spectrum(spectrumObject, xmesh, ymesh), columns=x, index=y)\n","\n","    if isinstance(spectrumObject, pd.DataFrame):\n","        dx = spectrumObject.columns[1] - spectrumObject.columns[0]\n","        dy = spectrumObject.index[1] - spectrumObject.index[0]\n","        xSpectrum = spectrumObject.sum(0) * dy\n","        ySpectrum = spectrumObject.sum(1) * dx\n","    else:\n","        xSpectrum = None\n","        ySpectrum = None\n","\n","    dataDx = data.columns[1] - data.columns[0]\n","    dataDy = data.index[1] - data.index[0]\n","\n","    axesRatios = np.array([data.columns[-1] - data.columns[0], data.index[-1] - data.index[0]])\n","    axesRatios = 3 * axesRatios / max(axesRatios)\n","    figureSize = figureScale * 2 * (1 + axesRatios) + np.array([2, 0])\n","\n","    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, constrained_layout=True, figsize=figureSize,\n","                                                 gridspec_kw={'height_ratios': [axesRatios[1], 1],\n","                                                              'width_ratios': [1, axesRatios[0]]})\n","\n","    plot(data, spectrumObject, axes=ax2, xLabel=None, yLabel=None)\n","    plot(data.sum(0) * dataDy, xSpectrum, axes=ax4, xLabel=r'$E_\\beta$ (keV)')\n","    dataGamma = data.sum(1) * dataDx\n","    ax1.errorbar(dataGamma.values, dataGamma.index, xerr=np.sqrt(dataGamma.values), fmt='.')\n","    if ySpectrum is not None:\n","        y = ySpectrum.index\n","        ax1.plot(ySpectrum.values, y)\n","    ax1.invert_xaxis()\n","    ax1.margins(0.05, 0)\n","    ax1.set_ylabel(r'$E_\\gamma$ (keV)')\n","    ax3.axis('off')\n","    ax3.text(0, 0, 'Counts / keV', rotation=0)\n","    ax4.set_ylabel('')\n","    return [fig, [[ax1, ax2], [ax3, ax4]]]\n","\n","\n","\n","import os\n","import re\n","import time\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def load_histogram(file_path):\n","    \"\"\"\n","    Unified loader for both MC (.phd) and real INL (.pbg) histograms.\n","    BOTH are 256×256 for your dataset.\n","    \"\"\"\n","\n","    with open(file_path, \"r\", errors=\"ignore\") as f:\n","        lines = [l.strip() for l in f.readlines()]\n","\n","    # Locate histogram block\n","    start = None\n","    stop = None\n","    for i, line in enumerate(lines):\n","        if \"#Histogram\" in line:\n","            start = i\n","        if (\"STOP\" in line) or (\"#Certificate\" in line):\n","            stop = i\n","            break\n","\n","    if start is None or stop is None:\n","        raise ValueError(\"Histogram block not found\")\n","\n","    hist_block = lines[start+1 : stop]\n","\n","    # First line contains metadata for real files (256 256 700 1000)\n","    # Determine whether this file has metadata based on how many numbers are in line 0.\n","    meta_vals = hist_block[0].split()\n","\n","    if len(meta_vals) == 4:\n","        # REAL format (.pbg)\n","        data_lines = hist_block[1:]\n","        expected_shape = (256, 256)\n","        file_type = \"real\"\n","    else:\n","        # MC format (.phd)\n","        data_lines = hist_block\n","        expected_shape = (256, 256)\n","        file_type = \"MC\"\n","\n","    # Convert to matrix\n","    mat = np.array([[float(v) for v in row.split()] for row in data_lines], dtype=np.float32)\n","\n","    # Validate shape\n","    if mat.shape != expected_shape:\n","        raise ValueError(f\"Unexpected histogram shape: {mat.shape}, expected: {expected_shape}\")\n","\n","    # Normalize\n","    total = mat.sum()\n","    if total > 0:\n","        mat /= total\n","\n","    return mat, {\"type\": file_type}\n","\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from scipy.spatial.distance import cosine\n","from scipy.stats import entropy\n","\n","# -----------------------------------------------------\n","# Preprocessing: log transform + normalize to probability\n","# -----------------------------------------------------\n","def preprocess_hist(mat):\n","    mat = np.asarray(mat, dtype=np.float32)\n","\n","    # ensure non-negative\n","    mat = np.clip(mat, 0, None)\n","\n","    # log transform to control big peaks\n","    mat = np.log1p(mat)\n","\n","    # Normalize to probability\n","    s = mat.sum()\n","    if s > 0:\n","        mat = mat / s\n","\n","    return mat\n","\n","\n","def preprocess_for_model(hist):\n","    \"\"\"Input: (256,256) or (256,256,1) histogram\n","       Output: processed histogram with same shape.\"\"\"\n","\n","    # Remove channel dimension if present\n","    if hist.ndim == 3:\n","        hist = hist[:, :, 0]\n","\n","    # clip negatives\n","    hist = np.clip(hist, 0, None)\n","\n","    # log transform\n","    hist = np.log1p(hist)\n","\n","    # normalize to total counts (probability map)\n","    s = hist.sum()\n","    if s > 0:\n","        hist = hist / s\n","\n","    return hist\n","\n","\n","\n"],"metadata":{"id":"4FL4KqZOeWuo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Change default plotting parameters\n","mpl.rcParams['image.cmap'] = 'bone_r'\n","mpl.rcParams['image.origin'] = 'lower'\n","plt.rcParams[\"figure.figsize\"] = (10, 10)\n","plt.rcParams['font.size'] = 14\n"],"metadata":{"id":"604pDfYTfL78"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Pre-process MC, PNNL Data, only need to run this once and then the data will be good to just recall from wherever it is stored --saves time\n","bob = TestAccumulator(WorkingDirectory)\n","bob.execute() #(WorkingDirectory)\n","\n","conclist = [result.concentration for result in bob.results]\n","histlist = [result.histogram for result in bob.results]\n","\n","histData = np.asarray(histlist, dtype=np.uint16)\n","concData = np.asarray(conclist, dtype=np.uint32)\n","\n","\n","save_path = os.path.join(WorkingDirectory, \"processed_results.npz\")\n","np.savez_compressed(save_path, histData=histData, concData=concData)\n","print(\"Saved:\", save_path)\n","\n","\n"],"metadata":{"id":"9VPEV7YFfNAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#To load in pre-processed MC Data\n","data = np.load(os.path.join(WorkingDirectory, \"processed_results.npz\"))\n","histData = data[\"histData\"]\n","concData = data[\"concData\"]"],"metadata":{"id":"yPn6XfcYfa7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Shared Set up and Preprocessing\n","#Prepare train/val/test splits and reshape\n","\n","# histData: (N, 256, 256) from your TestAccumulator\n","# concData: (N, 4)         isotope counts\n","\n","# cast to float32 for TF\n","histData_f = histData.astype(\"float32\")\n","#histData_f = np.array([preprocess_for_model(h) for h in histData], dtype=np.float32)\n","\n","concData_f = concData.astype(\"float32\")\n","\n","# single global train/test split\n","X_train_val, X_test, y_train_val, y_test = train_test_split(\n","    histData_f, concData_f, test_size=0.2, random_state=42\n",")\n","\n","# for CNN / Combined (need channel dimension)\n","X_train_val_img = X_train_val[..., np.newaxis]  # (N, 256, 256, 1)\n","X_test_img      = X_test[..., np.newaxis]\n","\n","# for NN (flat input)\n","X_train_val_flat = X_train_val.reshape(len(X_train_val), -1)  # (N, 256*256)\n","X_test_flat      = X_test.reshape(len(X_test), -1)\n","\n","# train/validation split\n","X_train_img, X_val_img, y_train, y_val = train_test_split(\n","    X_train_val_img, y_train_val, test_size=0.2, random_state=42\n",")\n","X_train_flat, X_val_flat, _, _ = train_test_split(\n","    X_train_val_flat, y_train_val, test_size=0.2, random_state=42\n",")\n","\n","# shared callbacks\n","redlr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, verbose=1)\n","early = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1)\n","\n","\n","def plot_history(history, title_prefix=\"\"):\n","    \"\"\"Plot loss and MAE for a Keras History object.\"\"\"\n","    h = history.history\n","    epochs = range(1, len(h[\"loss\"]) + 1)\n","\n","    plt.figure(figsize=(12,4))\n","\n","    # Loss\n","    plt.subplot(1,2,1)\n","    plt.plot(epochs, h[\"loss\"], label=\"train\")\n","    if \"val_loss\" in h:\n","        plt.plot(epochs, h[\"val_loss\"], label=\"val\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"MSE loss\")\n","    plt.title(f\"{title_prefix} Loss\")\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # MAE\n","    if \"mae\" in h:\n","        plt.subplot(1,2,2)\n","        plt.plot(epochs, h[\"mae\"], label=\"train\")\n","        if \"val_mae\" in h:\n","            plt.plot(epochs, h[\"val_mae\"], label=\"val\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"MAE\")\n","        plt.title(f\"{title_prefix} MAE\")\n","        plt.legend()\n","        plt.grid(True)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n"],"metadata":{"id":"sYsHyvg8frOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Training\n"],"metadata":{"id":"f7Qw986Miz5h"}},{"cell_type":"markdown","source":["Here is where you would change the loss to MSE or Huber.... also change where the model and outputs are being saved if you want to run both, so that the results for both are saved separately"],"metadata":{"id":"2_ugt-8kf2i8"}},{"cell_type":"code","source":["# ------------------------------------------------\n","# 2. NN model (flatten → dense layers)\n","# ------------------------------------------------\n","nn_input = layers.Input(shape=(256*256,))\n","x = layers.Dense(128, activation=\"relu\")(nn_input)\n","x = layers.Dense(64, activation=\"relu\")(x)\n","x = layers.Dense(32, activation=\"relu\")(x)\n","nn_output = layers.Dense(4, activation=\"linear\")(x)\n","\n","modelNN = models.Model(nn_input, nn_output)\n","modelNN.compile(optimizer=\"adam\", loss=Huber(delta=1.0), metrics=[\"mae\", \"mse\"])\n","modelNN.summary()\n","\n","historyNN = modelNN.fit(\n","    X_train_flat, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_data=(X_val_flat, y_val),\n","    callbacks=[redlr, early],\n","    verbose=1\n",")\n","\n","plot_history(historyNN, title_prefix=\"NN\")\n","\n","# predictions on test set if you want\n","test_predictionsNN = modelNN.predict(X_test_flat)\n"],"metadata":{"id":"MfnVbAkXf0ej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Saving ===\n","\n","# Save predictions\n","\n","np.save(\n","    os.path.join(WorkingDirectory, \"test_predictionsNN_new_loss.npy\"),\n","    test_predictionsNN,\n","    allow_pickle=True\n",")\n","\n","\n","# Save model\n","\n","save_dir = os.path.join(WorkingDirectory, \"saved_models_new_loss\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","modelNN.save(os.path.join(save_dir, \"modelNN_new_loss.keras\"))\n","\n","\n","# Save training history\n","\n","with open(os.path.join(WorkingDirectory, \"training_historyNN_new_loss.json\"), \"w\") as f:\n","    json.dump(historyNN.history, f)\n","\n","print(\"Model, predictions, and history saved successfully.\")\n"],"metadata":{"id":"XUyl81MHgGGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load in NN results if want to do analysis without retraining\n","\n","\n","\n","# Paths\n","model_path = os.path.join(WorkingDirectory, \"saved_models_new_loss\", \"modelNN_new_loss.keras\")\n","pred_path  = os.path.join(WorkingDirectory, \"test_predictionsNN_new_loss.npy\")\n","hist_path  = os.path.join(WorkingDirectory, \"training_historyNN_new_loss.json\")\n","\n","#load model\n","loaded_modelNN = tf.keras.models.load_model(model_path)\n","print(\"Loaded model:\", loaded_modelNN)\n","\n","#predictions\n","loaded_predictionsNN = np.load(pred_path, allow_pickle=True)\n","print(\"Loaded predictions shape:\", loaded_predictionsNN.shape)\n","\n","#training history\n","with open(hist_path, \"r\") as f:\n","    loaded_historyNN = json.load(f)\n","\n","print(\"Loaded history keys:\", loaded_historyNN.keys())\n"],"metadata":{"id":"Z_cB8UUfgRq4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#CNN Model\n","\n","cnn_input = layers.Input(shape=(256, 256, 1))\n","\n","c = layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(cnn_input)\n","c = layers.MaxPooling2D((2,2))(c)\n","c = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(c)\n","c = layers.MaxPooling2D((2,2))(c)\n","c = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(c)\n","c = layers.MaxPooling2D((2,2))(c)\n","\n","c = layers.Flatten()(c)\n","c = layers.Dense(128, activation=\"relu\")(c)\n","c = layers.Dense(64,  activation=\"relu\")(c)\n","cnn_output = layers.Dense(4, activation=\"linear\")(c)\n","\n","modelCNN = models.Model(cnn_input, cnn_output)\n","modelCNN.compile(optimizer=\"adam\", loss=Huber(delta=1.0), metrics=[\"mae\", \"mse\"])\n","modelCNN.summary()\n","\n","historyCNN = modelCNN.fit(\n","    X_train_img, y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_data=(X_val_img, y_val),\n","    callbacks=[redlr, early],\n","    verbose=1\n",")\n","\n","plot_history(historyCNN, title_prefix=\"CNN\")\n","\n","test_predictionsCNN = modelCNN.predict(X_test_img)\n"],"metadata":{"id":"i9SjjteRgc1v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Saving ===\n","\n","#save predictions\n","np.save(\n","    os.path.join(WorkingDirectory, \"test_predictionsCNN_new_loss.npy\"),\n","    test_predictionsCNN,   # <-- no need to wrap in list\n","    allow_pickle=True\n",")\n","\n","#save model\n","save_dir = os.path.join(WorkingDirectory, \"saved_models_new_loss\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","modelCNN.save(os.path.join(save_dir, \"modelCNN_new_loss.keras\"))  # <-- only one model object\n","\n","#save training history\n","with open(os.path.join(WorkingDirectory, \"training_historyCNN_new_loss.json\"), \"w\") as f:\n","    json.dump(historyCNN.history, f)   # <-- history is not a list anymore\n","\n","print(\"Model, predictions, and history saved successfully.\")\n"],"metadata":{"id":"Ico6Mqm-ghpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load in CNN\n","\n","model_path = os.path.join(WorkingDirectory, \"saved_models_new_loss\", \"modelCNN_new_loss.keras\")\n","pred_path  = os.path.join(WorkingDirectory, \"test_predictionsCNN_new_loss.npy\")\n","hist_path  = os.path.join(WorkingDirectory, \"training_historyCNN_new_loss.json\")\n","\n","#model\n","loaded_modelCNN = tf.keras.models.load_model(model_path)\n","print(\"Loaded model:\", loaded_modelCNN)\n","\n","#predictions\n","loaded_predictionsCNN = np.load(pred_path, allow_pickle=True)\n","print(\"Loaded predictions shape:\", loaded_predictionsCNN.shape)\n","\n","#training history\n","with open(hist_path, \"r\") as f:\n","    loaded_historyCNN = json.load(f)\n","\n","print(\"Loaded history keys:\", loaded_historyCNN.keys())\n"],"metadata":{"id":"mDTm5mjygsID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Combined model (CNN branch + direct image branch)\n","\n","# CNN branch\n","comb_cnn_input = layers.Input(shape=(256, 256, 1), name=\"cnn_input\")\n","z = layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(comb_cnn_input)\n","z = layers.MaxPooling2D((2,2))(z)\n","z = layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(z)\n","z = layers.MaxPooling2D((2,2))(z)\n","z = layers.Conv2D(128, (3,3), activation=\"relu\", padding=\"same\")(z)\n","z = layers.MaxPooling2D((2,2))(z)\n","z = layers.Flatten()(z)\n","\n","# \"existing data\" branch (right now also the histogram image)\n","comb_existing_input = layers.Input(shape=(256, 256, 1), name=\"existing_input\")\n","y_flat = layers.Flatten()(comb_existing_input)\n","\n","# Concatenate learned features + raw image info\n","combined = layers.concatenate([z, y_flat])\n","\n","# Dense head\n","h = layers.Dense(256, activation=\"relu\")(combined)\n","h = layers.Dense(128, activation=\"relu\")(h)\n","h = layers.Dense(64,  activation=\"relu\")(h)\n","comb_output = layers.Dense(4, activation=\"linear\")(h)\n","\n","modelCombined = models.Model(\n","    inputs=[comb_cnn_input, comb_existing_input],\n","    outputs=comb_output\n",")\n","modelCombined.compile(optimizer=\"adam\", loss=Huber(delta=1.0), metrics=[\"mae\", \"mse\"])\n","modelCombined.summary()\n","\n","historyCombined = modelCombined.fit(\n","    [X_train_img, X_train_img],  # both branches get the same data for now\n","    y_train,\n","    epochs=50,\n","    batch_size=32,\n","    validation_data=([X_val_img, X_val_img], y_val),\n","    callbacks=[redlr, early],\n","    verbose=1\n",")\n","\n","plot_history(historyCombined, title_prefix=\"Combined\")\n","\n","test_predictionsCombined = modelCombined.predict([X_test_img, X_test_img])\n"],"metadata":{"id":"NzD_-FnDg1Um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save Combined\n","\n","#save predictions\n","np.save(\n","    os.path.join(WorkingDirectory, \"test_predictionsCombined_new_loss.npy\"),\n","    test_predictionsCombined,   # <-- no need to wrap in list\n","    allow_pickle=True\n",")\n","\n","#save model\n","save_dir = os.path.join(WorkingDirectory, \"saved_models_new_loss\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","modelCombined.save(os.path.join(save_dir, \"modelCombined_new_loss.keras\"))  # <-- only one model object\n","\n","#save training history\n","with open(os.path.join(WorkingDirectory, \"training_historyCombined_new_loss.json\"), \"w\") as f:\n","    json.dump(historyCombined.history, f)   # <-- history is not a list anymore\n","\n","print(\"Model, predictions, and history saved successfully.\")\n"],"metadata":{"id":"JbpM7M3Ig8ex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Open Combined\n","\n","model_path = os.path.join(WorkingDirectory, \"saved_models_new_loss\", \"modelCombined_new_loss.keras\")\n","pred_path  = os.path.join(WorkingDirectory, \"test_predictionsCombined_new_loss.npy\")\n","hist_path  = os.path.join(WorkingDirectory, \"training_historyCombined_new_loss.json\")\n","\n","#load model\n","loaded_modelCombined = tf.keras.models.load_model(model_path)\n","print(\"Loaded model:\", loaded_modelCombined)\n","\n","#load predictions\n","loaded_predictionsCombined = np.load(pred_path, allow_pickle=True)\n","print(\"Loaded predictions shape:\", loaded_predictionsCombined.shape)\n","\n","#load training history\n","with open(hist_path, \"r\") as f:\n","    loaded_historyCombined = json.load(f)\n","\n","print(\"Loaded history keys:\", loaded_historyCombined.keys())\n"],"metadata":{"id":"vjLjF3kOhGtz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Evaluation"],"metadata":{"id":"q-i3mvv6h2EX"}},{"cell_type":"code","source":["#if using the loaded in models\n","modelNN      = loaded_modelNN\n","modelCNN     = loaded_modelCNN\n","modelCombined= loaded_modelCombined"],"metadata":{"id":"Woa-CRLvhOQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Concentration Plotting\n","isotope_idx = {\n","    \"Xe131m\": 0,\n","    \"Xe133\": 1,\n","    \"Xe133m\": 2,\n","    \"Xe135\": 3\n","}\n","\n","pred_CNN   = modelCNN.predict(X_test_img)\n","pred_NN    = modelNN.predict(X_test_flat)\n","pred_comb  = modelCombined.predict([X_test_img, X_test_img])\n","\n","for name, idx in isotope_idx.items():\n","\n","    plt.figure(figsize=(5,5))\n","    plt.scatter(y_test[:, idx], pred_CNN[:, idx],  alpha=0.3, label=\"CNN\")\n","    plt.scatter(y_test[:, idx], pred_NN[:, idx],   alpha=0.3, label=\"NN\")\n","    plt.scatter(y_test[:, idx], pred_comb[:, idx], alpha=0.3, label=\"Combined\")\n","\n","    plt.xlabel(\"True Concentration\")\n","    plt.ylabel(\"Predicted Concentration\")\n","    plt.title(f\"Prediction vs True — {name}\")\n","    plt.grid()\n","    plt.legend()\n","    plt.show()"],"metadata":{"id":"3J4ylhs5hxQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#confusion rates\n","\n","def confusion_rate(y_true, y_pred, threshold=0.005):\n","    \"\"\"\n","    Computes % of wrong-isotope predictions.\n","\n","    Wrong isotope = true = 0 AND predicted > threshold_value.\n","\n","    threshold: expressed as fraction of max true concentration.\n","    \"\"\"\n","    # Compute a dynamic threshold per isotope to ensure it scales with different magnitudes of concentrations\n","    # Count zero truth (when isotope is truly absent) cases and false positives\n","    max_per_isotope = np.max(y_true, axis=0)\n","    thresh_vals = threshold * max_per_isotope   # shape (4,)\n","\n","    N = len(y_true)\n","    wrong_counts = np.zeros(4)\n","    total_zeros  = np.zeros(4)\n","\n","    for iso in range(4):\n","        true_zero = (y_true[:, iso] == 0) #boolean mask\n","        total_zeros[iso] = np.sum(true_zero) #number of negatives\n","\n","        pred_wrong = y_pred[:, iso] > thresh_vals[iso] #predicts above threshold\n","        wrong_counts[iso] = np.sum(pred_wrong & true_zero) #flase positives\n","\n","    rate = wrong_counts / total_zeros\n","    return rate, wrong_counts, total_zeros, thresh_vals\n","\n","\n","# ---- Compute rates for all 3 models ----\n","rates_CNN, wrong_CNN, N0_CNN, threshCNN = confusion_rate(y_test, loaded_predictionsCNN)\n","rates_NN, wrong_NN, N0_NN, threshNN = confusion_rate(y_test, loaded_predictionsNN)\n","rates_COMB, wrong_COMB, N0_COMB, threshCOMB = confusion_rate(y_test, loaded_predictionsCombined)\n","\n","iso_names = [\"Xe131m\", \"Xe133m\", \"Xe133\", \"Xe135\"]\n","\n","print(\"\\n=== WRONG ISOTOPE CONFUSION RATES ===\")\n","for i, name in enumerate(iso_names):\n","    print(f\"\\n{name}:\")\n","    print(f\"  CNN:      {rates_CNN[i]*100:.2f}%   ({wrong_CNN[i]}/{N0_CNN[i]})\")\n","    print(f\"  NN:       {rates_NN[i]*100:.2f}%   ({wrong_NN[i]}/{N0_NN[i]})\")\n","    print(f\"  Combined: {rates_COMB[i]*100:.2f}%   ({wrong_COMB[i]}/{N0_COMB[i]})\")\n"],"metadata":{"id":"rQOYp29Uh98x"},"execution_count":null,"outputs":[]}]}